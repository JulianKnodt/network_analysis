{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35592\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "only one element tensors can be converted to Python scalars",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-08ab520105af>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[0mnum_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_Y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_Y\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0msplit_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_X\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#train_Y.shape,test_X.shape,test_Y.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[0mtrain_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_Y\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_X\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: only one element tensors can be converted to Python scalars"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import networkx as nx\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import random as rand\n",
    "import torch.nn.functional as F\n",
    "\n",
    "#total transactions =35592\n",
    "device = torch.device(\"cpu\")\n",
    "if torch.cuda.is_available():\n",
    "  device = torch.device(\"cuda:0\")\n",
    "NUM_EPOCH=5\n",
    "FEATURES = 8\n",
    "batch_size=512\n",
    "def normalize(v: [-10, 10]) -> [0,1]: return (v+10)/20\n",
    "\n",
    "# add incre to the [key1][key2] entry of \"dict\"\n",
    "def update(incre: int, dict, key1, key2):\n",
    "    if key1 in dict:\n",
    "        if key2 in dict[key1]:\n",
    "            dict[key1][key2]=dict[key1][key2]+incre\n",
    "        else:\n",
    "            dict[key1][key2]=incre\n",
    "    else: \n",
    "        dict[key1]={}\n",
    "        dict[key1][key2]=incre\n",
    "    return\n",
    "\n",
    "# return [key1][key2] entry of \"dict\" if the entry is nonempty\n",
    "def check_entry(dict, key1, key2):\n",
    "    if key1 in dict:\n",
    "        if key2 in dict[key1]:\n",
    "            return dict[key1][key2]\n",
    "    return 0\n",
    "#splits data into batches\n",
    "def split_data(featMat, labelVec):\n",
    "    num_batch = len(featMat)//batch_size\n",
    "    num_test=num_batch//4\n",
    "    num_train=num_batch-num_test\n",
    "    split_indicator=np.append(np.zeros(num_test, dtype=int),np.ones(num_train, dtype=int))\n",
    "    rand.shuffle(split_indicator)\n",
    "    train_X = []\n",
    "    train_Y = []\n",
    "    test_X = []\n",
    "    test_Y = []\n",
    "    for index in range(num_batch):\n",
    "        if split_indicator[index] == 0:\n",
    "            test_X.append(featMat[index*batch_size+1:(index+1)*batch_size,])\n",
    "            test_Y.append(labelVec[index*batch_size+1:(index+1)*batch_size])\n",
    "        else:\n",
    "            train_X.append(featMat[index*batch_size+1:(index+1)*batch_size,])\n",
    "            train_Y.append(labelVec[index*batch_size+1:(index+1)*batch_size])\n",
    "    return num_train, num_test, train_X, train_Y, test_X, test_Y\n",
    "                \n",
    "        \n",
    "        \n",
    "    \n",
    "def feature_vectors(max_node: int, src: [int], dst: [int], ratings: [float], phase_len=3000):\n",
    "  assert(len(src) == len(ratings))\n",
    "  assert(len(dst) == len(ratings))\n",
    "  print(len(src))\n",
    "  G = nx.DiGraph()\n",
    "  T = len(ratings)\n",
    "  rater_sums = {}\n",
    "  n_raters = {}\n",
    "  ratee_sums = {}\n",
    "  n_ratees = {}\n",
    "  total_ratings = 0\n",
    "  n_total_ratings = 0\n",
    "  epsilon = 1.e-17\n",
    "\n",
    "  phases = T//phase_len\n",
    "  G = nx.DiGraph()\n",
    "    \n",
    "  # feature vectors for each phase for each node\n",
    "  feats = torch.zeros(phase_len * phases, FEATURES, device=device, dtype=torch.float)\n",
    "  # How was this person rated on average before this phase?\n",
    "  labels = torch.zeros(phase_len * phases, device=device, dtype=torch.float)\n",
    "  nbhd_s = {}\n",
    "  n_nbhd = {}\n",
    "  for p in range(0, phases):\n",
    "    start = phase_len * p\n",
    "    rtr_sums = [0] * max_node\n",
    "    rte_sums = [0] * max_node\n",
    "    n_rtr = [0] * max_node\n",
    "    n_rte = [0] * max_node\n",
    "    for t in range(start, start+phase_len):\n",
    "      s,d,r = src[t], dst[t], ratings[t]\n",
    "      rtr_sums[s] += r\n",
    "      n_rtr[s] += 1\n",
    "      rte_sums[d] += r\n",
    "      n_rte[d] += 1\n",
    "      G.add_node(s)\n",
    "      G.add_node(d)\n",
    "      for nbr in G.neighbors(s):\n",
    "            update(r, nbhd_s, nbr, d)\n",
    "            update(1, n_nbhd, nbr, d)\n",
    "      G.add_edge(s, d)\n",
    "      #this should not create duplicates (see MultiDiGraph)\n",
    "      total_ratings += r\n",
    "      n_total_ratings += 1\n",
    "      feats[t, 0] = rtr_sums[s]/n_rtr[s]\n",
    "      feats[t, 1] = n_rtr[s]\n",
    "      feats[t, 2] = total_ratings/ n_total_ratings\n",
    "      feats[t, 3] = n_total_ratings\n",
    "      feats[t, 4] = rte_sums[d]/n_rte[d]\n",
    "      feats[t, 5] = n_rte[d]\n",
    "      feats[t, 6] = check_entry(nbhd_s,s,d)/(check_entry(n_nbhd,s,d)+epsilon)\n",
    "      feats[t, 7] = check_entry(n_nbhd,s,d)\n",
    "      labels[t] = r\n",
    "  return feats, labels\n",
    "\n",
    "def init_graph():\n",
    "  G = nx.DiGraph()\n",
    "  max_node = 0\n",
    "  with open(\"soc-sign-bitcoinotc.csv\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    for src, dst, w, time in reader:\n",
    "      G.add_node(src)\n",
    "      G.add_node(dst)\n",
    "      G.add_edge(src, dst, weight=normalize(w), time=time)\n",
    "      max_node = max(src, max(dst, max_node))\n",
    "  return G, max_node\n",
    "\n",
    "def get_vectors():\n",
    "  with open(\"soc-sign-bitcoinotc.csv\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    srcs = []\n",
    "    dsts = []\n",
    "    ratings = []\n",
    "    max_node = 0\n",
    "    for src, dst, w, time in reader:\n",
    "      src = int(src)\n",
    "      dst = int(dst)\n",
    "      w = float(w)\n",
    "      srcs.append(src)\n",
    "      dsts.append(dst)\n",
    "      ratings.append(normalize(w))\n",
    "      max_node = max(src, max(dst, max_node))\n",
    "  return srcs, dsts, ratings, max_node\n",
    "\n",
    "srcs, dsts, ratings, max_node = get_vectors()\n",
    "\n",
    "feats, labels = feature_vectors(max_node, srcs, dsts, ratings)\n",
    "\n",
    "# returns a feature matrix for all nodes in the graph\n",
    "def rater_summaries(G):\n",
    "  out = []\n",
    "  #path_lens = list(nx.all_pairs_dijkstra_path_length(G))\n",
    "  for n in G.nodes:\n",
    "    outgoing = G.adj[n]\n",
    "    avg_rating = 0\n",
    "    if len(outgoing) != 0:\n",
    "      avg_rating = sum(outgoing[x]['weight'] for x in outgoing)/len(outgoing)\n",
    "    out.append([\n",
    "      avg_rating,\n",
    "    ])\n",
    "  return out\n",
    "\n",
    "# A very basic predictor model with two linear layers.\n",
    "class Predictor(nn.Module):\n",
    "  def __init__(\n",
    "    self,\n",
    "    hidden_size=64,\n",
    "    features=5,\n",
    "    out = 1,\n",
    "  ):\n",
    "    super().__init__()\n",
    "    self.layer1 = nn.Linear(features, hidden_size)\n",
    "    self.layer2 = nn.Linear(hidden_size, out)\n",
    "    self.act = nn.LeakyReLU()\n",
    "  def forward(self, feat_vecs: [\"BATCH\", \"FEATURES\"]):\n",
    "    x = self.layer1(feat_vecs)\n",
    "    y = self.layer2(self.act(feat_vecs))\n",
    "    return y\n",
    "\n",
    "class Trainer():\n",
    "    def __init__(self,net=None,optim=None,loss_function=None, train_loader=None):\n",
    "        self.net = net\n",
    "        self.optim = optim\n",
    "        self.loss_function = loss_function\n",
    "        self.train_loader = train_loader\n",
    "\n",
    "    def train(self,epochs):\n",
    "        losses = []\n",
    "        for epoch in range(epochs):\n",
    "            epoch_loss = 0.0\n",
    "            epoch_steps = 0\n",
    "            for data in self.train_loader:              \n",
    "                X = data[0].to(device)\n",
    "                y = data[1].to(device)                               \n",
    "                self.optim.zero_grad()                \n",
    "                prediction = self.net(X)               \n",
    "                loss = self.loss_function(prediction, y)                \n",
    "                loss.backward()\n",
    "                self.optim.step()\n",
    "                epoch_loss += loss.item()\n",
    "                epoch_steps += 1            \n",
    "            losses.append(epoch_loss / epoch_steps)\n",
    "            print(\"epoch [%d]: loss %.3f\" % (epoch+1, losses[-1]))\n",
    "        return losses\n",
    "\n",
    "num_train, num_test, train_X, train_Y, test_X, test_Y= split_data(feats, labels)\n",
    "\n",
    "print(num_train,num_test,np.array(train_X).shape)#train_Y.shape,test_X.shape,test_Y.shape)\n",
    "\n",
    "train_loader = np.concatenate((train_Y,train_X), axis=1)\n",
    "test_loader = np.concatenate((test_Y,test_X), axis=1)\n",
    "net = ConvNet()\n",
    "net = net.to(device) \n",
    "#opt = optim.SGD(net.parameters(), lr=0.1, momentum=0.9)\n",
    "loss_function = nn.pairwise_distance()\n",
    "\n",
    "trainer = Trainer(net=net, loss_function=loss_function, train_loader=train_loader)\n",
    "\n",
    "losses = trainer.train(NUM_EPOCH)\n",
    "\n",
    "assert(losses[-1] < 0.03)\n",
    "assert(len(losses)==num_epochs)\n",
    "\n",
    "# Given some model, and a node \"n\", as well as a graph G which has prior ratings and timestamps,\n",
    "# and outputs a predicted \"trust\" in [0,1].\n",
    "def predict_trust(model, n, G):\n",
    "  ...\n",
    "\n",
    "# Given some model, and a node \"n\", as well as a graph G which has prior ratings and timestamps,\n",
    "# and outputs a predicted \"local trust\" based on its neighbors ratings in [0,1].\n",
    "def predict_local_trust(model, n, G):\n",
    "  ...\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
